{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12677baf",
   "metadata": {},
   "source": [
    "1. Input from user\n",
    "2. Local llm\n",
    "3. LLM provides structured output for a function to be called\n",
    "4. We call the function\n",
    "5. We take the output of the function and feed that back into the llm\n",
    "6. the llm provides an output for the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f0a895a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompt_from_user = \"\"\"\n",
    "What is Lucas profession, favorite movie and favorite book??\n",
    "Use the information from the file: lucas_secrets.txt\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68ae3b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lucas is an AI engineer. His favorite movie is \"In Bruges\" by Martin Macdonaugh\\nand his favorite book is \"Ethics a Nichomacea\" by Aristotle.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_lucas_info(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        return file.read()\n",
    "    \n",
    "get_lucas_info(file_path=\"./lucas_secrets.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b51c0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'function': {'name': 'get_lucas_info', 'arguments': {'file_path': 'lucas_secrets.txt'}}}]\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(\n",
    "    model='mistral-small3.2',\n",
    "    messages=[{'role': 'user', 'content': \n",
    "        input_prompt_from_user}],\n",
    "\n",
    "    tools=[{\n",
    "      'type': 'function',\n",
    "      'function': {\n",
    "        'name': 'get_lucas_info',\n",
    "        'description': 'Get the information about Lucas',\n",
    "        'parameters': {\n",
    "          'type': 'object',\n",
    "          'properties': {\n",
    "            'file_path': {\n",
    "              'type': 'string',\n",
    "              'description': 'The path to the file containing the information about Lucas',\n",
    "            },\n",
    "          },\n",
    "          'required': ['file_path'],\n",
    "        },\n",
    "      },\n",
    "    }],\n",
    ")\n",
    "\n",
    "print(response['message']['tool_calls'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a2b1af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lucas is an AI engineer. His favorite movie is \"In Bruges\" by Martin Macdonaugh\\nand his favorite book is \"Ethics a Nichomacea\" by Aristotle.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def execute_tool_call(tool_call_output):\n",
    "    \n",
    "    if tool_call_output[0]['function']['name'] == 'get_lucas_info':\n",
    "        return get_lucas_info(tool_call_output[0]['function']['arguments']['file_path'])\n",
    "    else:\n",
    "        return \"Tool not found\"\n",
    "\n",
    "tool_call_output = response['message']['tool_calls']    \n",
    "output_of_function =execute_tool_call(tool_call_output)\n",
    "output_of_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8942d647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'mistral-small3.2', 'created_at': '2025-07-30T16:16:27.38742Z', 'message': {'role': 'assistant', 'content': 'Based on the information from the file `lucas_secrets.txt`, here is the answer to your query:\\n\\n- **Profession**: Lucas is an AI engineer.\\n- **Favorite Movie**: \"In Bruges\" by Martin McDonagh.\\n- **Favorite Book**: \"Ethics: Nicomachean\" by Aristotle.'}, 'done_reason': 'stop', 'done': True, 'total_duration': 4321344000, 'load_duration': 45447584, 'prompt_eval_count': 590, 'prompt_eval_duration': 933953500, 'eval_count': 68, 'eval_duration': 3340000167}\n"
     ]
    }
   ],
   "source": [
    "new_input_with_more_information = f\"\"\"\n",
    "The initial query was: {input_prompt_from_user}.\n",
    "The output of the function was: {output_of_function}.\n",
    "Provide the answer:\n",
    "\"\"\"\n",
    "response = ollama.chat(\n",
    "    model='mistral-small3.2',\n",
    "    messages=[{'role': 'user', 'content': \n",
    "        new_input_with_more_information}],\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8cef69fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Lucas is an AI engineer. His favorite movie is \"In Bruges\" by Martin Macdonaugh\n",
       "and his favorite book is \"Ethics a Nichomacea\" by Aristotle."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(f\"# {output_of_function}\"))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
