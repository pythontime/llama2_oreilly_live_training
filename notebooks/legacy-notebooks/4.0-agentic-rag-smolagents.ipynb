{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic RAG with SmoLAgents\n",
    "\n",
    "This notebook demonstrates how to build agentic RAG systems using HuggingFace's SmoLAgents library.\n",
    "\n",
    "## Contents\n",
    "1. Quickstart - Basic agent usage\n",
    "2. Agentic RAG - Advanced retrieval with reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "First, let's install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'smolagents[toolkit]' pandas langchain langchain-community sentence-transformers datasets python-dotenv rank_bm25 --upgrade -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Quickstart Examples\n",
    "\n",
    "### Example 1: Basic Agent\n",
    "\n",
    "Let's create a simple agent that can perform calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import CodeAgent, InferenceClientModel\n",
    "\n",
    "# Initialize a model\n",
    "model = InferenceClientModel()\n",
    "\n",
    "# Create an agent with no tools\n",
    "agent = CodeAgent(tools=[], model=model)\n",
    "\n",
    "# Run the agent with a task\n",
    "result = agent.run(\"Calculate the sum of numbers from 1 to 10\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Agent with Web Search Tool\n",
    "\n",
    "Now let's add web search capabilities to our agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import CodeAgent, InferenceClientModel, DuckDuckGoSearchTool\n",
    "\n",
    "model = InferenceClientModel()\n",
    "agent = CodeAgent(\n",
    "    tools=[DuckDuckGoSearchTool()],\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "# Agent can now search the web\n",
    "result = agent.run(\"What are the latest developments in LLM agents?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Using Different Models\n",
    "\n",
    "SmoLAgents supports various model backends:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a specific Hugging Face model\n",
    "from smolagents import InferenceClientModel\n",
    "\n",
    "# You can specify any model from Hugging Face\n",
    "model = InferenceClientModel(model_id=\"Qwen/Qwen2.5-72B-Instruct\")\n",
    "\n",
    "agent = CodeAgent(tools=[], model=model)\n",
    "result = agent.run(\"What is the Fibonacci sequence? Calculate the first 10 numbers.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Agentic RAG Implementation\n",
    "\n",
    "Now let's build a more advanced system: an agentic RAG pipeline that can reason over documents.\n",
    "\n",
    "### What is Agentic RAG?\n",
    "\n",
    "Traditional RAG:\n",
    "1. User query → Retrieve documents → Generate answer\n",
    "\n",
    "Agentic RAG:\n",
    "1. User query → Agent formulates optimal retrieval query\n",
    "2. Agent retrieves documents\n",
    "3. Agent reasons over results\n",
    "4. Agent may retrieve again with refined queries\n",
    "5. Agent synthesizes final answer\n",
    "\n",
    "### Step 1: Load and Prepare Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load Hugging Face documentation dataset\n",
    "print(\"Loading knowledge base...\")\n",
    "knowledge_base = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\")\n",
    "\n",
    "# Filter for transformers documentation\n",
    "knowledge_base = knowledge_base.filter(\n",
    "    lambda row: row[\"source\"].startswith(\"huggingface/transformers\")\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(knowledge_base)} documents\")\n",
    "\n",
    "# Convert to Document objects\n",
    "source_docs = [\n",
    "    Document(\n",
    "        page_content=doc[\"text\"],\n",
    "        metadata={\"source\": doc[\"source\"].split(\"/\")[1]}\n",
    "    )\n",
    "    for doc in knowledge_base\n",
    "]\n",
    "\n",
    "# Split documents into chunks for better retrieval\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "docs_processed = text_splitter.split_documents(source_docs)\n",
    "print(f\"Split into {len(docs_processed)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create Custom Retriever Tool\n",
    "\n",
    "We'll create a custom tool that the agent can use to retrieve relevant documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import Tool\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "class RetrieverTool(Tool):\n",
    "    name = \"retriever\"\n",
    "    description = \"Uses semantic search to retrieve documents that match the query. Returns the top k most relevant documents.\"\n",
    "    inputs = {\n",
    "        \"query\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The search query to find relevant documents\"\n",
    "        }\n",
    "    }\n",
    "    output_type = \"string\"\n",
    "\n",
    "    def __init__(self, docs, k=10, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.retriever = BM25Retriever.from_documents(docs, k=k)\n",
    "\n",
    "    def forward(self, query: str) -> str:\n",
    "        \"\"\"Retrieve documents based on the query.\"\"\"\n",
    "        docs = self.retriever.invoke(query)\n",
    "        \n",
    "        # Format retrieved documents\n",
    "        result = \"\\n\\n\".join(\n",
    "            [f\"Document {i+1}:\\n{doc.page_content}\" for i, doc in enumerate(docs)]\n",
    "        )\n",
    "        return result\n",
    "\n",
    "# Initialize the retriever tool\n",
    "retriever_tool = RetrieverTool(docs_processed, k=7)\n",
    "print(\"Retriever tool created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create the Agentic RAG System\n",
    "\n",
    "Now we'll create an agent that can use the retriever tool to answer questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import CodeAgent, InferenceClientModel\n",
    "\n",
    "# Initialize the model\n",
    "model = InferenceClientModel()\n",
    "\n",
    "# Create agent with retriever tool\n",
    "agent = CodeAgent(\n",
    "    tools=[retriever_tool],\n",
    "    model=model,\n",
    "    max_steps=4,\n",
    "    verbosity_level=2  # Set to 2 to see agent's reasoning process\n",
    ")\n",
    "\n",
    "print(\"Agentic RAG system ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Test the Agentic RAG System\n",
    "\n",
    "Let's ask some questions about the Transformers library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Simple question\n",
    "question = \"How do I load a pretrained model in Transformers?\"\n",
    "print(f\"Question: {question}\\n\")\n",
    "\n",
    "answer = agent.run(question)\n",
    "print(f\"\\nAnswer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: More complex question requiring multi-step reasoning\n",
    "question = \"What are the different methods for fine-tuning models, and which one should I use for limited computational resources?\"\n",
    "print(f\"Question: {question}\\n\")\n",
    "\n",
    "answer = agent.run(question)\n",
    "print(f\"\\nAnswer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Question that might require multiple retrievals\n",
    "question = \"Compare AutoModel and AutoTokenizer - how do they work together and what are their key differences?\"\n",
    "print(f\"Question: {question}\\n\")\n",
    "\n",
    "answer = agent.run(question)\n",
    "print(f\"\\nAnswer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benefits of Agentic RAG\n",
    "\n",
    "The agentic approach provides several advantages:\n",
    "\n",
    "1. **Adaptive Retrieval**: The agent can formulate multiple queries if needed\n",
    "2. **Reasoning**: The agent can analyze retrieved content and synthesize information\n",
    "3. **Self-Correction**: If initial results aren't satisfactory, the agent can try different approaches\n",
    "4. **Multi-Step Queries**: Complex questions can be broken down into sub-questions\n",
    "\n",
    "Compare this to traditional RAG where you have a fixed pipeline with no ability to adapt based on the quality of retrieved results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Using Local Models\n",
    "\n",
    "You can also use local models with SmoLAgents. Here's an example with Ollama:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to use with Ollama (requires Ollama to be running)\n",
    "# from smolagents import LiteLLMModel\n",
    "# \n",
    "# model = LiteLLMModel(model_id=\"ollama/llama3.1\", api_base=\"http://localhost:11434\")\n",
    "# \n",
    "# agent = CodeAgent(\n",
    "#     tools=[retriever_tool],\n",
    "#     model=model,\n",
    "#     max_steps=4,\n",
    "#     verbosity_level=2\n",
    "# )\n",
    "# \n",
    "# answer = agent.run(\"How do I use pipeline in Transformers?\")\n",
    "# print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Try extending this notebook by:\n",
    "1. Adding your own documents to the knowledge base\n",
    "2. Creating custom tools for specific tasks\n",
    "3. Experimenting with different retrieval methods (dense embeddings, hybrid search)\n",
    "4. Adding memory to the agent for multi-turn conversations\n",
    "5. Implementing citation tracking to show which documents were used"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
